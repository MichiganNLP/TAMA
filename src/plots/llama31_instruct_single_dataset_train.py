import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import seaborn as sns
from plots.utils.utils import set_parameters_heatmap, NumberFontSize, plot_details, FigSize


# Example Data for one train-test pair
data_0 = {
    "train_dataset": ["FeTaQA"] * 30 + ["HiTab"] * 30 + ["TabFact"] * 30,
    "test_dataset": ["TabFact"] * 90,
    "learning_rate": [x for x in [1.00E-05] * 6 + [5.00E-06] * 6 + [1.00E-06] * 6 + [5.00E-07] * 6 + [1.00E-07] * 6] * 3,
    "epochs": [1, 2, 3, 4, 5, 6] * 15,
    "performance": [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.01565067689,0,0,0,0,0,0,0,0,0,0,0,56.94498787,51.42029893,51.03685734,55.84161515,44.26011425,53.61921903,62.10188591,59.67603099,60.4116128,52.57844902,60.90460912,60.56029423,65.74066828,67.7987323,68.33868065,68.83950231,68.76124892,68.51083809,45.85648329,60.59159559,60.81070506,57.7040457,54.84779717,54.3861022,0,0,0,0,0,0,50.38735425,50.44995696,50.03521402,50.62211441,51.90546991,51.8819939,62.00798185,69.36379998,68.83167697,69.14469051,72.27482589,72.39220596,72.51741138,73.23734252,73.45645199,74.66937945,74.71633148,74.71633148,68.89427968,70.10720714,71.86008295,72.2983019,72.64261679,72.32177792,0,65.70936693,66.87534236,67.10227717,67.72047891,67.66570154],
}

data_1 = {
    "train_dataset": ["FeTaQA"] * 30 + ["HiTab"] * 30 + ["TabFact"] * 30,
    "test_dataset": ["HiTab"] * 90,
    "learning_rate": [x for x in [1.00E-05] * 6 + [5.00E-06] * 6 + [1.00E-06] * 6 + [5.00E-07] * 6 + [1.00E-07] * 6] * 3,
    "epochs": [1, 2, 3, 4, 5, 6] * 15,
    "performance": [1.578282828,1.452020202,1.957070707,1.388888889,1.704545455,1.515151515,2.462121212,1.893939394,1.704545455,2.02020202,2.20959596,2.335858586,1.830808081,1.767676768,1.641414141,1.388888889,1.578282828,1.199494949,4.924242424,2.651515152,1.452020202,1.136363636,1.073232323,1.452020202,0.4419191919,5.429292929,7.765151515,7.512626263,7.765151515,7.386363636,53.59848485,51.57828283,56.12373737,58.6489899,56.50252525,57.95454545,61.36363636,58.39646465,60.85858586,61.30050505,63.63636364,63.57323232,59.34343434,64.77272727,64.83585859,65.21464646,64.83585859,65.21464646,56.25,59.78535354,61.36363636,61.48989899,61.80555556,62.87878788,9.532828283,44.88636364,47.79040404,48.35858586,48.67424242,47.91666667,0.06313131313,0,0,0.06313131313,0,0,31.43939394,34.28030303,36.67929293,33.96464646,34.1540404,34.46969697,54.04040404,55.61868687,55.61868687,57.13383838,56.12373737,56.18686869,48.5479798,48.35858586,48.67424242,50,50.69444444,50.69444444,0.1893939394,13.57323232,16.09848485,17.42424242,17.92929293,18.62373737],
}


data_2 = {
    "train_dataset": ["FeTaQA"] * 30 + ["HiTab"] * 30 + ["TabFact"] * 30,
    "test_dataset": ["FeTaQA"] * 90,
    "learning_rate": [x for x in [1.00E-05] * 6 + [5.00E-06] * 6 + [1.00E-06] * 6 + [5.00E-07] * 6 + [1.00E-07] * 6] * 3,
    "epochs": [1, 2, 3, 4, 5, 6] * 15,
    "performance": [32.63700027,34.84245434,35.89529508,35.52247591,36.50249818,37.60909931,34.17102104,36.78828004,36.12198329,36.3698362,37.84535752,37.89529277,33.75296731,35.12033074,35.25880737,36.06444171,34.97725479,35.02200757,30.34768134,32.75739607,33.82003501,34.78564975,34.55356513,34.01906033,18.85718373,26.39794907,27.93299758,29.17511388,29.02138536,29.65171659,0.6292441866,0.5842427724,1.114317078,0.4267107306,0.8157068842,0.5552799908,1.982174398,4.599928824,2.841901751,1.746508663,1.123296906,1.664153506,1.547578739,4.210950216,4.440196364,2.591731841,3.406953825,3.68100241,4.877187047,2.456034168,2.567461387,3.626640414,5.297348106,5.300802783,17.17591137,21.44780659,19.53390616,18.42718419,16.90359662,16.6980191,1.71E-10,1.83E-10,1.53E-10,1.55E-10,1.45E-10,1.46E-10,0.001046017513,0.0004209001875,7.19E-05,8.42E-06,6.94E-07,2.80E-06,5.081770486,2.14626619,2.333631655,2.092149569,2.078087693,1.977902871,8.774843107,7.644283372,8.362800068,7.580327613,7.612381555,7.166351037,15.55177675,19.42261408,19.92256902,20.29052618,20.59272188,20.27579954],
}


data_3 = {
    "train_dataset": ["FeTaQA"] * 30 + ["HiTab"] * 30 + ["TabFact"] * 30,
    "test_dataset": ["FEVEROUS"] * 90,
    "learning_rate": [x for x in [1.00E-05] * 6 + [5.00E-06] * 6 + [1.00E-06] * 6 + [5.00E-07] * 6 + [1.00E-07] * 6] * 3,
    "epochs": [1, 2, 3, 4, 5, 6] * 15,
    "performance": [0,0,0,0,0,0,0,0,0,0,0,0,9.031505251,0.1400233372,0.1633605601,0.1400233372,0.2800466744,0.186697783,5.250875146,5.950991832,1.213535589,1.446907818,0.9568261377,1.236872812,1.330221704,3.687281214,5.064177363,5.134189032,4.527421237,4.154025671,59.55659277,53.27887981,58.94982497,58.83313886,56.59276546,58.90315053,64.83080513,66.25437573,67.74795799,68.61143524,67.70128355,67.957993,70.38506418,72.22870478,72.08868145,72.27537923,72.48541424,72.41540257,69.03150525,69.0781797,70.1050175,70.03500583,70.33838973,70.68844807,4.55075846,52.36872812,54.88914819,56.49941657,59.06651109,60.37339557,51.62193699,46.93115519,48.2613769,52.08868145,51.7386231,52.69544924,57.78296383,70.66511085,71.06184364,71.87864644,72.78879813,73.04550758,71.64527421,73.83897316,73.93232205,74.65577596,74.4924154,74.42240373,70.15169195,71.36522754,73.25554259,74.04900817,73.76896149,73.65227538,11.43523921,68.82147025,68.14469078,68.07467911,69.45157526,68.61143524],
}


data_4 = {
    "train_dataset": ["FeTaQA"] * 30 + ["HiTab"] * 30 + ["TabFact"] * 30,
    "test_dataset": ["MMLU"] * 90,
    "learning_rate": [x for x in [1.00E-05] * 6 + [5.00E-06] * 6 + [1.00E-06] * 6 + [5.00E-07] * 6 + [1.00E-07] * 6] * 3,
    "epochs": [1, 2, 3, 4, 5, 6] * 15,
    "performance": [65.12,64.57,63.73,63.17,63.17,63.24,66.32,66.19,65.66,65.72,65.66,65.62,65.78,65.75,65.79,65.82,65.77,65.73,65.82,65.75,65.77,65.75,65.79,65.79,65.95,65.98,65.9,65.93,65.89,65.96,64.82,63.55,62.91,62.87,62.34,62.38,67.44,67.12,66.77,66.61,66.7,66.51,66.07,66.22,66.37,66.66,66.58,66.56,65.98,66.06,66.08,66.05,66.19,66.19,66.15,66.2,66.17,66.12,66.07,66.05,30,29.34,29.95,33.73,35.37,35.82,64.3,62.85,64.51,64.63,64.77,64.76,66.21,66.33,66.74,66.71,66.69,66.75,66.13,66.09,66.31,66.33,66.46,66.42,66.22,65.95,66.15,66.07,66.07,66.03],
}


data_5 = {
    "train_dataset": ["FeTaQA"] * 30 + ["HiTab"] * 30 + ["TabFact"] * 30,
    "test_dataset": ["IFEval"] * 90,
    "learning_rate": [x for x in [1.00E-05] * 6 + [5.00E-06] * 6 + [1.00E-06] * 6 + [5.00E-07] * 6 + [1.00E-07] * 6] * 3,
    "epochs": [1, 2, 3, 4, 5, 6] * 15,
    "performance": [27.69784173,29.13669065,31.41486811,31.89448441,29.3764988,30.45563549,44.48441247,50.47961631,53.35731415,58.63309353,54.43645084,54.67625899,75.7793765,76.61870504,77.8177458,74.82014388,75.53956835,76.73860911,76.25899281,77.69784173,77.8177458,78.53717026,79.13669065,77.69784173,78.29736211,80.81534772,79.13669065,79.73621103,78.29736211,78.65707434,37.29016787,34.05275779,29.73621103,34.77218225,32.37410072,32.9736211,61.03117506,53.47721823,54.55635492,57.55395683,57.91366906,59.47242206,79.49640288,79.97601918,78.17745803,78.77697842,80.45563549,79.13669065,80.69544365,81.29496403,79.97601918,79.97601918,79.49640288,79.49640288,79.49640288,81.05515588,79.73621103,81.29496403,79.49640288,77.57793765,26.01918465,25.89928058,25.65947242,24.70023981,25.41966427,25.29976019,45.32374101,52.0383693,49.40047962,47.00239808,43.52517986,45.68345324,80.33573141,77.57793765,77.69784173,77.8177458,76.37889688,78.41726619,79.61630695,79.73621103,79.73621103,80.57553957,80.69544365,79.13669065,79.3764988,78.77697842,78.41726619,79.73621103,79.97601918,80.81534772],
}


# given a learning rate and a epoch, we need to get the performance corresponding to train and test domains

df = pd.DataFrame(data_0)

# Get unique values for epochs and learning rates
unique_epochs = df['epochs'].unique()[:3]
unique_learning_rates = df['learning_rate'].unique()

# Create subplots with the number of rows and columns based on unique_learning_rates and unique_epochs
nrows = len(unique_learning_rates)
ncols = len(unique_epochs)
fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(24, 16))
plt.subplots_adjust(hspace=0.35, wspace=0.3)  # Reduce space between subplots

set_parameters_heatmap()

performance_min = 0
performance_max = 100

for i, learning_rate_filter in enumerate(unique_learning_rates):
    for j, epoch_filter in enumerate(unique_epochs):
        
        dfs = []
        for data in [data_0, data_1, data_2, data_3, data_4, data_5]:
            df = pd.DataFrame(data)
            filtered_df = df[(df['learning_rate'] == learning_rate_filter) & (df['epochs'] == epoch_filter)]
            dfs.append(filtered_df)

        filtered_df = pd.concat(dfs)

        heatmap_data = filtered_df.pivot_table(values='performance', index='train_dataset', columns='test_dataset')


        sns.heatmap(heatmap_data, annot=True, cmap="YlGnBu", ax=axes[i][j], cbar_kws={'label': 'Performance'}, square=True, fmt=".2f", cbar=False, vmin=performance_min, vmax=performance_max)

        axes[i][j].set_title(f"LR: {learning_rate_filter}, Epoch: {epoch_filter}")

        ax = axes[i][j]
        ax.tick_params(axis='x', rotation=0)  # Rotate x-ticks
        ax.tick_params(axis='y', rotation=0)  # Rotate y-ticks

        # Set x and y labels
        ax.set_xlabel('Test')
        ax.set_ylabel('Train')
    

fig.colorbar(plt.cm.ScalarMappable(cmap="YlGnBu", norm=plt.Normalize(vmin=performance_min, vmax=performance_max)), ax=axes, location='right', shrink=0.6, label='Performance')

plt.savefig(f"../figures/single_task_all_3.pdf")
plt.close()
